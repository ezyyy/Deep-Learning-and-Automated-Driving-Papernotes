# [GAN Dissection: Visualizing and Understanding Generative Adversarial Networks](https://arxiv.org/abs/1811.10597)

**Problem**: Learning generative models using GANs has recently brought produced impressive results in different vision tasks. However, GANs have not been well visualized or understood. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models.

**Solution**:
* The authors develop an analytical framework to visualize and understand GANs at the unit-, object-, and scene-level.
* They propose two techniques: dissection and intervention. In a first step, the authors argue that each intermediate layer r contains all the information necessary to construct an output image
* **Dissection** 
* **Intervention**

**Notes**:
* Even though the problem of exaplainability in GANs relates to various tasks, the authors only come up with a solution for vision. The paper also lacks transfer to other domains.

