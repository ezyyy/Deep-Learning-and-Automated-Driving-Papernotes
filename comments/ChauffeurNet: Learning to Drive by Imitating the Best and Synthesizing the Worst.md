# [ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst](https://arxiv.org/abs/1812.03079)

**Problem**: *Adversarial Examples* are imperceptibly perturbed natural inputs that induce erroneous predictions in state-of-the-art classifiers. Why they exist has been subject of heavy debate, but existing explanations (i.e. aberrations arising either from the high dimensional nature of the input space or statistical fluctuations in the training data) are often unable to fully capture behaviors observed in practice.

**Solution**: Use [imitation learning](https://blog.statsbot.co/introduction-to-imitation-learning-32334c3b1e7a) (on 30 million driving examples) to train a policy for autonomous driving that is robust enough to drive a real vehicle 

**Notes**:
* 
