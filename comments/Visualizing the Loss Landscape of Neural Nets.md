# [Visualizing the Loss Landscape of Neural Nets](https://arxiv.org/abs/1712.09913)

**Problem**: Typically, loss functions of neural networks are highly non-convex and therefore possess many different local minima an optimizer can get stuck in - in fact, what one finds in general is a particular local minimum that generalizes well (see, e.g. (here)[https://stats.stackexchange.com/questions/106334/cost-function-of-neural-network-is-non-convex].
**Solution**: 

**Notes**:
* Preface: On a side note: For large-size networks, most local minima are equivalent and yield similar performance on a test set (see (this paper)[https://arxiv.org/pdf/1412.0233v3.pdf]




